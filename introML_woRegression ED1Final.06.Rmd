---
title: "ML intro"
output:
  html_notebook:
    code_folding: none
    df_print: paged
    highlight: tango
---

<style type= "text/css">
body { font-size: 15px; }
code.r { font-size: 15px; }
</style>

This code is copyright Alces Ronin Presciient Pty. Ltd. 2018 and comes with no warranties whatsoever.
It is for instructional purposes only and NOT for implementation commercially or otherwise.

Prepare packages and adjust some settings

```{r}
if (!"pacman" %in% rownames(installed.packages())) install.packages("pacman")
if (!"parsnip" %in% rownames(installed.packages())) {
  devtools::find_rtools()
  devtools::install_github("topepo/parsnip")
}

pacman::p_load(
  tidyverse, tidymodels, dials, parsnip, magrittr, lubridate, pryr,
  devtools, rattle.data, ranger, glmnet, rpart, rpart.plot)
set.seed(115)
```

## Getting familiar with the data

```{r}
data(weatherAUS)
weather <- list()
weather$aus <- weatherAUS %>% 
  as.tibble() %T>%  # this pipe operator allows to execute the following statement 
  # ignoring its effect on the whole pipeline
  print()
```

What's is structure, are there any missing values?

```{r}
summary(weather$aus)
```

Lots of missing values.

Let's focus on Canberra

```{r}
weather$canberra <- 
  weather$aus %>% 
  filter(Location == "Canberra") %>% 
  arrange(Date) %T>% 
  print()

```

## Missing value imputation for time series

Summarise the amount of missing values

```{r}
weather$canberra %>% 
  map_int(~sum(is.na(.))) %>% 
  keep(~. > 0) %>% 
  as_tibble() %>% 
  rownames_to_column("col_name")
```

Missing value data processing:
1. create a missing value indicator with a tally of the recency of the missing value
2. impute the most recent non-missing value  

Missing value indicators for missing time series data

```{r}
# a counter matrix, each column contains a number of subsequent NAs in a row
miss_tally <- matrix(0, nrow(weather$canberra), ncol(weather$canberra)) %>% 
  as_tibble() %>% 
  rename_all(~str_c(names(weather$canberra), "_mistally"))

# to fill subsequent lines the first one must not contain NAs
stopifnot(weather$canberra[1, ] %>% is.na() %>% not() %>% all())

# initialise the first row
miss_tally[1, ] <- rep(0, ncol(weather$canberra))

for (i in 2:nrow(weather$canberra)) {
  # the mask of NAs for the current row
  mis_mask <- is.na(weather$canberra[i, ])[1, ]
  
  # update the mis_mask value tallies for all fields
  miss_tally[i, mis_mask] <- miss_tally[i - 1, mis_mask] + 1
}
```

An imputed version of weather$canberra with previous values

```{r}
weather$canber_imput <- weather$canberra %>% tidyr::fill(everything())
```

Check miss_tally

```{r}
stopifnot(all(
  is.na(weather$canberra) == (miss_tally > 0)))
```

Check imputed data set

```{r}
stopifnot(weather$canber_imput %>% is.na() %>% any() %>% not())
```

What does the imputed time series look like?

```{r}
summary(weather$canber_imput)
```
Missing does not necessarily just mean we failed to collect data.
It can also be a useful predictor in its own right.
We can add the missing value tallies as additional variables
First, get rid of fields with no missing

```{r}
cols_with_na <- weather$canberra %>% 
  # the same as map_lgl(function(x) any(is.na(x)))
  # _lgl asserts that output will be logical
  map_lgl(~any(is.na(.)))

weather$canber_imput_tally <- bind_cols(
  weather$canber_imput, 
  miss_tally[, cols_with_na])
```


Turn wind direction fields back into non-imputed fields because we will impute them a different way.

```{r}
weather$wind <- bind_cols(
  weather$canber_imput_tally, 
  weather$canberra %>% 
    select(contains("Dir")) %>% 
    rename_all(funs(str_c(., "_mistally"))))
```

Check the amount of missing values now

```{r}
weather$wind %>% 
  summarise_all(~sum(is.na(.))) %>% 
  simplify() %>%  # works similar to unlist
  # keeps only more than zero
  keep(~. > 0) %>% 
  print()
```

Restore original y-value fields -- we won't want to predict with missing y values,
even if they are imputed

```{r}
y_name <- "RainTomorrow"
y_related_names <- c("RISK_MM", y_name)

# keep non-imputed y-fields: we do NOT want to predict missing values.
weather$wind[, y_related_names] <- weather$canberra[, y_related_names]

weather$wind[, str_c(y_related_names, "_copy")] <- weather$wind[, y_related_names] %>% 
  rename_at(y_related_names, str_c, "_copy")
```

A small note.  
There are different versions of ```rename``` function:
- ```rename_all``` will rename all features using a provided function
- ```rename_at``` expects some description of column names. Character, index position, ...
- ```rename_if``` expects a function like ```is.numeric``` to select columns by content

The same logic is applicable for ```select```, ```mutate```, ```filter```, ```summarise``` 
functions in dplyr.


Find which fields have NAs, and which are factors

```{r}
has_na_col <- map_lgl(weather$wind, ~any(is.na(.)))

numeric_na_names <- 
  (map_lgl(weather$wind, is.numeric) & has_na_col) %>% 
  # allows to use boolean values as a mask for itself keeping only true values
  keep(~.) %>% 
  names()

factor_na_names  <- 
  (map_lgl(weather$wind, is.factor) & has_na_col) %>% 
  keep(~.) %>% 
  names()

numeric_na_names
```

Extract the non-directional fields

```{r}
weather$wo_dir <- weather$wind %>% 
  select(-contains("Dir"))  # component1: imputed numerics
```

Wind directions as simple categorics -- imputing "missing" as a category
Dates are ignored  (as in introductory course)
This is because date is just an ID - a single value per record,
with new, unrepeated values for new records.

```{r}
weather$cyc <- weather$wo_dir %>% 
  # remove RISK_MM and its tally: we won't be using this y value here - we will be using
  # RainToday. Also remove the Data and Location
  select(-contains("RISK_MM"), -Date, -Location)

weather$cyc[, str_c(factor_na_names, "_cat_imput")] <- 
  weather$wind %>% 
  select(factor_na_names) %>% 
  # add NAs as a new category. map_dfc() ensures that the result will be a dataframe
  map_dfc(fct_explicit_na)
```

Print levels of the first column of factor_na_names after the transformation

```{r}
weather$cyc %>% 
  select(str_c(factor_na_names, "_cat_imput")) %>% 
  first() %>% 
  levels()
```

Investigate the final version of the data

```{r}
str(weather$cyc)
```

We have a rare target class

```{r}
# write.csv(weather$cyc, "weatherCategoricImputed.csv")  # write out the data
weather$cyc %>% count(RainToday)
```

Let's do some machine learning.  
The ```modelr``` package from ```tidymodels``` proposes a "tidy" idea of separation of
model description (called "specification") and actual learning with the ```fit()``` function.
The latter allows you to use the same model on different data or with different packages
like ```ranger``` or ```randomForest```. We will stick with the same concept in our helper
functions.

Here is probability forest specification

```{r}
prob_forest_spec <- function(...)
  rand_forest(
    ..., mode = "classification", 
    others = list(probability = T, importance = "permutation"))
```


Here we fit the model based on the description

```{r}
prob_forest <- function(data, targ_formula = RainTomorrow ~ ., spec = NULL) {
  (if (is.null(spec)) prob_forest_spec() else spec) %>%
    # translate(engine = "ranger")
    fit(targ_formula, data = data, engine = "ranger") %>% 
    # adds the target column to the fitted model for predictions()
    `$<-`(y, data[[as.character(targ_formula)[[2]]]])
}

try(prob_forest(weather$cyc))
```

Oops, we have missing values in the data in the y values! Let's get rid of those records
and build a model once again

```{r}
weather$cyc2 <- weather$cyc %>% 
  select(-RainTomorrow_copy) %>% 
  filter(!is.na(get(y_name)))

# it will be a list of our models
forest <- list(cyc2 = prob_forest(weather$cyc2))
```

Get correct labels and OOB predictions from random forest

```{r}
y <- weather$cyc2$RainTomorrow
```

Based on a type we get predictions with this helper

```{r}
predictions <- function(mdl = NULL, new_data = NULL, type = "oob") {
  pred_probs <- if (type == "oob") {
    stopifnot(is.null(new_data))
    truth <- mdl$y
    mdl$fit$predictions
  } else {
    truth <- new_data$RainTomorrow
    predict(mdl, new_data, type = type) %>% 
      as_tibble() %>% 
      rename_all(funs(str_replace(., "\\.pred_", "")))
  }
  format_predictions(pred_probs, truth)
}
```

```{r}
format_predictions <- function(pred_probs, truth, estim_thresh = 0.5) {
  as_tibble(pred_probs) %>% 
    # some measure metrics go crazy absolute 0 and 1 predicted probablities
    mutate_at(c("No", "Yes"), ~case_when(. == 0 ~ 0.001, . == 1 ~ 0.999, TRUE ~ .)) %>% 
    mutate(prob = Yes) %>% 
    mutate(estimate = as.factor(if_else(prob >= estim_thresh, "Yes", "No"))) %>% 
    bind_cols(truth = truth) %>% 
    mutate(truth_num = as.numeric(truth) - 1)
}

y_hat2 <- predictions(forest$cyc2)
y_hat2
```

We will assess predictions with the ```yardstick::metrics()``` function

```{r}
assess_basic <- function(y_hat) {
  bind_cols(
    metrics(y_hat, truth = truth, estimate = estimate, No, Yes),
    metrics(y_hat, truth = truth_num, estimate = prob)) %>% 
    gather("metric") %>% 
    # yardstick implementation of logLoss is negative
    mutate_at("value", ~abs(round(., 3)))
}
```

Measure the model using all metrics above

```{r}
assess_basic(y_hat2)
```

Wow! those are perfect results!  Only one small problem...  
These are too good to be true!  
How could this be?  
Lets look at feature importances

```{r}
forest_importance <- function(forest, round_val = 3) {
  forest$fit$variable.importance %>% 
    as_tibble() %>% 
    rownames_to_column("feature") %>% 
    rename(importance = value) %>% 
    arrange(-importance) %>% 
    mutate_at("importance", round, round_val)
}

forest$cyc2 %>% forest_importance()
```


Variable importances show that the best predictor is something that should not be there at all 
-- two copies of the y value!
This is how it works in the real world, people! 
Sometimes we find out about problems only when we build the model  

Get rid of those fields 

```{r}
weather$cyc3 <- weather$cyc2 %>%
  select(-RainTomorrow_cat_imput,-RainTomorrow_copy_cat_imput)  # -RainTomorrow_copy_cat_imput
```

Fix factor_na_names for later stages of feature engineering - get rid of the y variable

```{r}
factor_na_names <- setdiff(factor_na_names, "RainTomorrow")
```

Re-run the modelling and error measurement

```{r}
forest$cyc3 <- prob_forest(weather$cyc3)

y_hat3 <- predictions(forest$cyc3)

assess_basic(y_hat3)
```

Good... but maybe we can do better!  

Let's build a more flexible function that can evaluate measures 
we haven't even created yet.  
More simple, flexible and general reporting function

```{r}
assess <- function(y_hat, extra_funs = extra_measures) {
  bind_rows(
    assess_basic(y_hat),
    extra_funs %>% 
      map_dbl(~.(y_hat)) %>% 
      map_dfc(round, 3) %>% 
      gather("metric"))
}
```

New error functions

```{r}
extra_measures <- list(
  # the same as function(x) with(x, cor(truth_num, prob))
  truth_pred_cor = . %>% na.omit() %$% cor(truth_num, prob),
  # "subtract" is a magrittr helper for "a - b" => subtract(a, b)
  max_error = . %>% na.omit() %$% subtract(truth_num, prob) %>% abs() %>% max())

extra_measures
```

Here the combination of a dot and a pipeline operator creates a functional sequence - 
a handy shortcut to create functions

```{r}
forest$cyc3 %>% predictions() %>% assess()
```

## Dates and factor columns 

Can we do better than categorical encoding of wind directions and excluding dates?  
Yes we can: wind directions have an order to them!  
We are also failing to use the date variable -- that could be quite useful too.  
Time of year should be informative regarding the incidence of rain  

Wind directions as ordinals (integers)  
Convert ordinal factors to numerics

```{r}
weather$cyc_ord_fct <- weather$wind %>% 
  select(factor_na_names) %>% 
  # for each column replace NA with "0" and reorder it: levels() -> c("0", other levels)
  # note, anonymous function with pipe don't start with "~" operator
  map_dfc(. %>% fct_explicit_na("0") %>% fct_relevel("0")) %>% 
  rename_all(~str_c(factor_na_names, "_ordinal"))
```

Date variable as an ordinal

```{r}
year_day <- weather$wind$Date %>% ymd() %>% yday()
days_in_year <- ifelse(weather$wind$Date %>% ymd() %>% leap_year(), 366, 365)
year_proportion <- year_day / days_in_year
```

```{r}
weather$cyc_ord <- bind_cols(
  weather$wo_dir, weather$cyc_ord_fct, year_proportion = year_proportion) %>% 
  filter(!is.na(RainTomorrow)) %>% 
  select(-contains("RISK_MM"), -RainTomorrow_copy,-RainTomorrow_copy_ordinal) %>%
  # get rid of missing y values
  select(-Date, -Location)

# write.csv(weather$cyc_ord, "weather_cyc_ord.csv")
```

Let's do some machine learning again

```{r}
forest$cyc_ord <- prob_forest(weather$cyc_ord)
```

Now wouldn't it be nice to compare reports?
This function contains a very useful pipeline that let us easily run multiple
instances of assess() which we wrote earlier, and combine them into a tibble

```{r}
assess_report <- function(models, data_versions = weather) {
  models %>% 
    # some data versions can be less than original. We need a match with "y"
    map(predictions) %>% 
    map(assess) %>% 
    # here .x means the orig df, .y is its name in the "models" list
    imap(~rename(.x, !!.y := value)) %>% 
    reduce_right(left_join, by = "metric")
}

assess_report(forest)
```

## Cyclic features

Wind and date as cyclical variables. Use trigonometry to create continuous, unique representations of cyclical fields.
Convert numerics to cyclicals

```{r}
cyclise <- function(col, col_name) {
  col <- as.numeric(col)
  n <- max(col, na.rm = T)
  
  tibble(
    !!str_c(col_name, "_sin") := sin(2*pi*col/n),
    !!str_c(col_name, "_cos") := cos(2*pi*col/n)) %>% 
    map_dfc(replace_na, 0)
}

weather$trig <- weather$canber_imput_tally %>% 
  select(contains("Dir")) %>% 
  bind_cols(year_proportion = year_proportion) %>% 
  imap(cyclise) %>% 
  bind_cols(weather$wo_dir) %T>% 
  print()

# write.csv(weather$trig, "weatherTrig.csv")

```

Does this help?

```{r}
weather$trig2 <- weather$trig %>% 
  filter(!is.na(RainTomorrow)) %>% 
  select(-contains("RISK_MM"), -RainTomorrow_copy) %>%
  select(-Date, -Location)

forest$trig2 <- prob_forest(weather$trig2)
```


```{r}
forest$cyc2 <- NULL
assess_report(forest)
```


Hmmmm. Maybe there is some value in using the categoric AND trignometric?

```{r}
weather$trig_cat <- weather$cyc3 %>% 
  select(contains("Dir")) %>% 
  bind_cols(weather$trig2)

forest$trig_cat <- prob_forest(weather$trig_cat)

assess_report(forest)
```

Now, time to notice that our data is a TIME SERIES.  
As such, we have a TOPOLOGY in the data: there is a notion of DISTANCE between records
apart from their distance in N-dimensional space.  
This is their distance in TIME.  
We can use the time dimension to enrich our data.  
More specifically, we are currently using the current day's 
data to predict the next day's rainfall.  
But what if we used more history: say the last three days?  
This is a standard trick with time series, called taking LAGS.  

A function to create a lagged version of the data.  

```{r}
lagged_tbl <- function(tbl, lag_n) {
  0:lag_n %>% 
    # creates "lag_n + 1" tibbles with different lags and combines them
    map_dfc(~mutate_all(tbl, lag, .)) %>% 
    # changes column names "a1b2" -> "a1b_lag2"
    rename_all(funs(str_replace(., "(\\d)$", "_lag\\1")))
}

tibble(a=1:3, b=letters[1:3]) %>% lagged_tbl(2)
```

This function helps us to reuse common pipeline in the future

```{r}
lagged_weather <- function(tbl, lag_n) {
  tbl %>% 
    mutate(rainfall_mm = RISK_MM_copy) %>%  # rename the copy fields
    lagged_tbl(lag_n) %>%  # apply the lag function
    drop_na() %>%  # get rid of NA valued y field elements
    select(-contains("RISK_MM"), -rainfall_mm) %>%  # left rainfall_mm lag only
    select(-contains("RainTomorrow_"), -Date, -Location)
}
```

Let's try lag 1

```{r}
weather$trig_lag <- weather$trig %>% 
  select(-contains("_mistall")) %>% 
  lagged_weather(lag_n = 1)

forest$trig_lag <- prob_forest(weather$trig_lag) 

assess_report(forest)
```

Let's just lag the rainfall

```{r}
weather$rain_lag <- weather$trig %>% 
  drop_na() %>% 
  lagged_weather(lag_n = 3) %>% 
  select(-contains("lag"), contains("rainfall_mm"))

forest$rain_lag <- prob_forest(weather$rain_lag) 

assess_report(forest)
```

What were the biggest predictors?

```{r}
biggest_preds <- forest$rain_lag %>%
  forest_importance() %T>%
  print() %>%
  select(feature) %>%
  head() %>% 
  dplyr::combine() # combine the most important only
```

Let's lag the biggest predictors

```{r}
weather$big_preds <- weather$trig %>% 
  drop_na() %>%  # drop_na() before any select()'s as for "rain_lag" for alignment
  select(one_of(biggest_preds)) %>% 
  lagged_tbl(lag_n = 3) %>%  # apply the lag function!
  drop_na() %>%  # get rid of the first lines with NAs as a result of lags
  select(-one_of(biggest_preds)) %>%  # only lagged verstions of biggest_preds
  bind_cols(weather$rain_lag)

forest$big_preds <- prob_forest(weather$big_preds) 

assess_report(forest)
```

## Important features and PCA

Pick the most important features

```{r}
ggplot(forest_importance(forest$big_preds, round_val = 4), 
       aes(x = reorder(feature, importance), y = importance)) + 
  geom_count(stat = "identity") + 
  coord_flip()
```

Reorder the fields according to importance

```{r}
cols <- list(
  imp = forest_importance(forest$big_preds, round_val = 4) %>% 
    arrange(-importance) %>% 
    select(feature) %>% 
    dplyr::combine())
```

What if we fit a model with different numbers of the most important features

```{r}
weather$lst_imp_feats <- purrr::set_names(c(5, seq.int(from = 10, to = 50, by = 10))) %>% 
  map(~weather$big_preds[c(cols$imp[1:.x], "RainTomorrow")])

weather$lst_imp_feats %>% 
  map(prob_forest) %>% 
  assess_report(data_versions = weather$lst_imp_feats)
```

Looks like we can keep the first 20 without noticeable decrease in the metric.

```{r}
weather$imp20 <- weather$lst_imp_feats$`20`
```

What if we don't throw away the rest of features?  
We could try pca shrinkage for the rest.

This helper selects numeric features with the number of distinct features > 2

```{r}
select_num <- . %>% select_if(~is.numeric(.) & n_distinct(.) > 2)
```

```{r}
dummy_matrix <- function(data, tbl = TRUE) {
  res <- model.matrix(~ ., data)[, -1]
  if (tbl) as_tibble(res) else res
}
```

```{r}
fit_pca <- function(data, sorted_imps, n_remain_features) {
  data %>% 
    select(sorted_imps[(n_remain_features + 1):length(sorted_imps)]) %>% 
    select_num() %>% 
    dummy_matrix() %>% 
    princomp(cor = T)
}
```

Check our hypothesis

```{r}
n_remain_features <- 20
n_best_components <- 5

pca_model <- fit_pca(weather$big_preds, cols$imp, n_remain_features)
  
weather$pca_shrink <- bind_cols(
  weather$big_preds[c(cols$imp[1:n_remain_features], "RainTomorrow")], 
  as_tibble(pca_model$scores[, 1:n_best_components]))

forest$pca_shrink <- prob_forest(weather$pca_shrink)

assess_report(forest)
```

We can even try different combinations of features to remain and the number of components

```{r}
purrr::set_names(c(20, 30)) %>% 
  map(~{
    n_remain_features <- .x
    pca <- fit_pca(weather$big_preds, cols$imp, n_remain_features)
    
    purrr::set_names(c(5, 10)) %>% 
      map(~{
        n_best_components <- .x
        shrink_df <- bind_cols(
          weather$big_preds[c(cols$imp[1:n_remain_features], "RainTomorrow")], 
          as_tibble(pca$scores[, 1:n_best_components]))
        
        prob_forest(shrink_df) %>% predictions() %>% assess()
      })
  }) %>% 
  unlist(recursive = F) %>% 
  imap(~rename(.x, !!.y := value)) %>% 
  reduce_right(left_join, by = "metric") %>% 
  rename_all(~str_replace(., "\\.", " n_feats|n_comps:"))
```


## Train-test split and cross-validation

To understand why we use out-of-bag predictions lets build a bunch of simple decision
trees with different depth on a whole dataset first

```{r}
trees_depth <- purrr::set_names(1:30) %>% 
  map(~rpart(
    RainTomorrow ~ ., data = weather$imp20, 
    # cp defines a minimum improvement in some splitting criteria
    # minsplit - the minumum number of objects in a node to be split
    maxdepth = ., cp = 0, minsplit = 2, model = TRUE))
```

Visualize a couple of them

```{r}
rpart.plot(trees_depth[[3]])
rpart.plot(trees_depth[[5]])
```

Plot accuracy vs tree depth

```{r}
trees_depth %>% 
  # get accuracy score for models
  map_dbl(~{
    predict(., weather$imp20) %>% 
      as_tibble() %>% 
      format_predictions(weather$imp20$RainTomorrow) %>% 
      assess() %>% 
      filter(metric == "accuracy") %>% 
      pull(value)
  }) %>% 
  # reorganize accuracy scores for different values of depth to a tibble
  enframe(value = "accuracy") %>% 
  mutate(name = as.numeric(name)) %>% 
  ggplot(aes(x = name, y = accuracy)) + geom_point()
```

To avoid a situtaion when we assess a model on the same data that was used we split 
the data to train and test sets.  
  
These helpers assess a bunch of models and creates a tibble report 

```{r}
tuning_report <- function(models, tune_param, split, need_oob = F) {
  models %>% 
    map(assess_split, split = split, need_oob = need_oob) %>% 
    unlist(recursive = F) %>% 
    imap(~mutate(.x, col = .y)) %>% 
    reduce(bind_rows) %>% 
    separate(col, c(tune_param, "mode")) %>% 
    mutate(!!tune_param := as.numeric(get(tune_param))) %>% 
    `class<-`(., c("tuning_report", class(.)))
}

assess_split <- function(mdl, split, need_oob = FALSE) {
  purrr::set_names(c("training", "testing", if (need_oob) "oob")) %>% 
    map(~{
      is_oob <- . == "oob"
      new_data <- if (!is_oob) get(.)(split)
      mdl %>% 
        predictions(new_data = new_data, type = if (is_oob) "oob" else "prob") %>% 
        assess()
    })
}
```

Split the data and generate assess reports for a tree parameter "maxdepth"

```{r}
weather_split <- initial_split(weather$imp20)

tuning <- list(rpart = list(
  depth = purrr::set_names(1:30) %>% 
    map(~rpart(
      RainTomorrow ~ ., training(weather_split), maxdepth = ., cp = 0, minsplit = 2)) %>% 
    tuning_report(tune_param = "maxdepth", split = weather_split)))

print(tuning$rpart$depth)
```

Things are going to be much clearer with a plot

```{r}
plot.tuning_report <- function(report, flip_x = FALSE) {
  # we want accuracy to be higher and logLoss to be lower. Split metrics on such groups
  high_better_metrics <- c("accuracy", "roc_auc", "rsq", "truth_pred_cor")
  pd <- position_dodge(0.1)
  # if we had multiple scores we can draw confidence intervals
  draw_ci <- all(c("low", "high") %in% names(report))
  if (!"value" %in% names(report)) report <- report %>% rename(value = mean)
  
  # plot high_better and low_better plots as two separate groups
  map(c(F, T), ~{
    x_name <- setdiff(head(names(report), 3), c("metric", "value", "mode"))
    metric_names <- unique(report$metric)
    draw_metrics <- metric_names[(metric_names %in% high_better_metrics) != .]
    
    report %>% 
      filter(metric %in% draw_metrics) %>% 
      ggplot(aes(y = value, x = get(x_name), color = mode)) + 
      { if (draw_ci) geom_errorbar(aes(ymin=low, ymax=high), width=0.5, position = pd) } +
      geom_line(stat="identity", alpha=0.1, size=1, position = pd) +
      geom_point(size = 2, position = pd) + 
      facet_grid(metric ~ ., scales = "free_y") + 
      xlab(x_name) + 
      { if (flip_x) scale_x_reverse() } + 
      ggtitle(sprintf("%s-better metrics vs %s", if (.) "Lower" else "Higher", x_name)) +
      theme(plot.title = element_text(hjust = 0.5))
  })
}

plot(tuning$rpart$depth)
```

How about another tree parameter - "minsplit"?

```{r}
tuning$rpart$minsplit <- tuning_report(
  purrr::set_names(1:50 * 2) %>% 
    map(~rpart(RainTomorrow ~ ., training(weather_split), cp = 0, minsplit = .)),
  tune_param = "minsplit", split = weather_split)

# to reflect growth of model complexity we flip x axis
plot(tuning$rpart$minsplit, flip_x = TRUE)
```

To get more robust scores we repeat the same procedure several times

```{r}
sampled_tuning_report <- function(sampled_reports_lst, tune_param = NULL) {
  if (is.null(tune_param)) {
    tune_param <- setdiff(names(sampled_reports_lst[[1]]), c("metric", "value", "mode"))
  }
  sampled_reports_lst %>% 
    # combine tuning reports into one tibble
    reduce(left_join, by = c("metric", "mode", tune_param)) %>% 
    # here "scores" in each cell contains a vector of sampled scores
    mutate(scores = pmap(select(., starts_with("value")), c)) %>% 
    select(metric, !!tune_param, mode, scores) %>% 
    # calculate means for sampled scores
    mutate(mean = map_dbl(scores, mean)) %>% 
    # low and high bounds of confidence inverval
    mutate(
      low_high = map(scores, ~{ 
        if (length(unique(.x)) == 1) .x[1:2] else t.test(.x)$conf.int 
      })) %>% 
    # split "low_high" to two separate columns
    `[<-`(, c("low", "high"), map(transpose(.$low_high), unlist)) %>% 
    `class<-`(., c("tuning_report", class(.)))
}
```

Run the previous experiment with resampling for "maxdepth"

```{r}
n_repeats <- 10

tuning$rpart$robust_minsplit <- 
  map(1:n_repeats, ~{
    weather_split <- initial_split(weather$imp20)
    cat(.)
    purrr::set_names(1:30) %>% 
      map(~rpart(
        RainTomorrow ~ ., training(weather_split), cp = 0, maxdepth=., minsplit = 2)) %>% 
      tuning_report(tune_param = "maxdepth", split = weather_split)
  }) %>% 
  sampled_tuning_report()

tuning$rpart$robust_minsplit %T>% 
  print() %>% 
  plot()
```

Another way of getting robust scores is cross-validation when we split the data on "k"
chunks and treat each chunk as test set. The rest is our train set. 
  
Check it for random forest.  
For each train set we can calculate out-of-bag estimates as well.
  
In order to generate a list of evenly spread values for a tuning parameter we will use 
```grid_regular()``` function from another ```tidymodels``` package called ```dials```

```{r}
rf_grid <- grid_regular(
  # range_set(trees, c(2, 200)),
  # range_set(mtry, c(3, ncol(weather$imp20) - 1)),
  range_set(min_n, c(2, 30)),
  levels = 15)

# to force map() iterate a tibble by rows not columns as usual
by_row <- . %>% split(seq(nrow(.)))

rf_specs <- rf_grid %>% 
  by_row() %>% 
  # create model specifications for every combination of tuned parameters
  map(merge, prob_forest_spec()) %>% 
  purrr::set_names(rf_grid$min_n)

cv_splits <- vfold_cv(weather$imp20, strata = "RainTomorrow", v = 10)
```

Fit these models

```{r}
tuning$forest$cv_nmin <- cv_splits$splits %>% 
  imap(~{
    cv_split <- .
    cat(.y)
    rf_specs %>% 
      map(~prob_forest(data = training(cv_split), spec = .)) %>% 
      tuning_report(tune_param = "min_n", split = cv_split, need_oob = T)
  }) %>% 
  sampled_tuning_report()
```

```{r}
tuning$forest$cv_nmin %T>% 
  print() %>% 
  plot(flip_x = T)
```


## Generalized linear models and cross validation

Will try a linear model.
One great feature of random forest is its out-of-bag predictions. 
Linear models don't have this opportunity so we going to use cross-validation 
to measure performance

```{r}
cv_glm <- function(data, verbose = FALSE, alpha = 0.5) {
  x <- select(data, -RainTomorrow)
  if (not(all(map_lgl(data, is.numeric)))) x <- dummy_matrix(x, tbl = FALSE)
  model <- cv.glmnet(x = x, y = data$RainTomorrow, nfolds = 10, alpha = alpha,
                     family = "binomial", type.measure = "auc", keep = T)
  if (verbose) {
    plot(model)
    print(max(model$cvm))
  }
  invisible(model)
}

cv_glm(last(weather), verbose = TRUE)
```

This is a helper to score our data with cv_glm model

```{r}
cv_glm_score <- function(data, alpha = 0.5) cv_glm(data, alpha = alpha)$cvm %>% 
  max() %>% round(4)
```

 checking different version of the data with resampling

```{r}
system.time({
  weather[purrr::set_names(c("cyc3", "big_preds", "pca_shrink"))] %>% 
    imap_dfc(~{
      data <- .x
      cat(.y, " ")
      1:5 %>% map_dbl(~cv_glm_score(data)) %>% mean()
    }) %>% 
    gather() %>% 
    print()
})
```

```big_preds``` seems like the best  version of ```weather``` for glm.
Lets tune better alpha value

```{r}
seq.int(from = 0, to = 1, by = 0.1) %>%  # alpha values
  purrr::set_names() %>% 
  imap_dfc(~cv_glm_score(weather$big_preds, alpha = .x)) %>% 
  gather(key = "alpha", value = "roc_auc") %$%
  qplot(alpha, roc_auc) %>% 
  print()
```

All values of alpha looks pretty good except alpha = 0

The last thing to try will be glm interations. We will use ```recipes``` package to 
construct a chain of data preparation steps.

```recipe()``` describes roles (predictor, outcome, other) in your data

```{r}
rec <- recipe(RainTomorrow ~ ., data = weather$big_preds)
rec
```

Then we apply different ```recipes::step_...()``` to the recipe object. We can use 
roles of features in steps or their types like ```all_nominal()```

```{r}
steps <- list(rec %>% step_dummy(all_nominal(), -all_outcomes()))
steps[[1]]
```

We can execute this step

```{r}
last(steps) %>% 
  # retain=T allows to apply it with "juice()" on the same data
  # juice() alternative is bake(newdata=..)
  prep(training = weather$big_preds, retain = TRUE)
```

```big_preds``` has only one nominal predictor with 2 levels so it hasn't affected the 
number of columns.  
Lets add another step with interactions

```{r}
steps[[2]] <- steps[[1]] %>% step_interact(~all_predictors():all_predictors())
```

Execute the step and check the number of columns now

```{r}
weather$interact <- last(steps) %>% 
  prep(training = weather$big_preds, retain = TRUE) %>%  
  juice()

ncol(weather$interact)
tail(names(weather$interact))
```

What is the new auc score?

```{r}
system.time(print(cv_glm_score(weather$interact)))
```

